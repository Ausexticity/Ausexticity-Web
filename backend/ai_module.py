import os
import json
from dotenv import load_dotenv
from google.cloud import bigquery
from vertexai.language_models import TextEmbeddingModel
import vertexai
import anthropic
from google.cloud import translate_v2 as translate
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

# åˆå§‹åŒ– Vertex AI
vertexai.init(project="eros-ai-446307", location="us-central1")

# åˆå§‹åŒ–ç¿»è­¯å®¢æˆ¶ç«¯
translate_client = translate.Client()

# åˆå§‹åŒ– ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=10)  # æ ¹æ“šéœ€æ±‚èª¿æ•´ max_workers

# åˆå§‹åŒ– Logger
logger = logging.getLogger('uvicorn.error')

def load_query_embedding_sync(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    model = TextEmbeddingModel.from_pretrained(model_name)
    embeddings = model.get_embeddings([query])
    return embeddings[0].values

async def load_query_embedding_async(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆéåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        load_query_embedding_sync,
        query,
        model_name
    )

def retrieve_similar_documents_sync(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    åŒæ­¥ä½¿ç”¨ BigQuery æŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    client = bigquery.Client(project=project_id)

    # å°‡æŸ¥è©¢å‘é‡è½‰æ›ç‚ºå­—ç¬¦ä¸²æ ¼å¼
    embedding_str = ", ".join(map(str, query_embedding))

    # å‰µå»º SQL æŸ¥è©¢ï¼Œä½¿ç”¨è¡¨åˆ¥åä¾†é¿å…æ¬„ä½åç¨±æ¨¡ç³Š
    query = f"""
    WITH query AS (
      SELECT
        [{embedding_str}] AS embedding
    )

    SELECT
      t.id,
      t.title,
      t.url,
      t.content,
      t.embedding,
      (
        (SELECT SUM(q * d) 
         FROM UNNEST(qry.embedding) AS q WITH OFFSET i 
         JOIN UNNEST(t.embedding) AS d WITH OFFSET j 
         ON i = j)
        /
        (SQRT((SELECT SUM(POWER(q, 2)) FROM UNNEST(qry.embedding) AS q)) * 
         SQRT((SELECT SUM(POWER(d, 2)) FROM UNNEST(t.embedding) AS d)))
      ) AS cosine_similarity
    FROM
      `{project_id}.{dataset_id}.{table_id}` AS t,
      query AS qry
    ORDER BY
      cosine_similarity DESC
    LIMIT {top_n}
    """

    query_job = client.query(query)
    results = query_job.result()

    documents = []
    for row in results:
        documents.append({
            "id": row.id,
            "title": row.title,
            "url": row.url,
            "content": row.content,
            "cosine_similarity": row.cosine_similarity
        })

    client.close()
    return documents

async def retrieve_similar_documents_async(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ BigQuery æŸ¥è©¢ï¼ŒæŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        retrieve_similar_documents_sync,
        query_embedding,
        dataset_id,
        table_id,
        project_id,
        top_n
    )

def translate_text_sync(text):
    """
    åŒæ­¥ä½¿ç”¨ Google Translate API å°‡æ–‡æœ¬ç¿»è­¯ç‚ºè‹±æ–‡ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    try:
        result = translate_client.translate(text, target_language='en')
        return result['translatedText']
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—ï¼š{str(e)}")
        return ""

async def translate_text_async(text):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ç¿»è­¯å‡½å¼ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        translate_text_sync,
        text
    )

def generate_response(documents_cn, documents_en, user_query, additional_context, claude_client, model):
    """
    ä½¿ç”¨æª¢ç´¢åˆ°çš„ä¸­è‹±æ–‡æ–‡æª”èˆ‡ç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆå›ç­”ã€‚

    Args:
        documents_cn (list): ä¸­æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        documents_en (list): è‹±æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        user_query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        additional_context (str): é¡å¤–çš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚
        claude_client: Anthropic çš„ API å®¢æˆ¶ç«¯ã€‚
        model (str): Anthropic æ¨¡å‹åç¨±ã€‚

    Returns:
        str: ç”Ÿæˆçš„å›ç­”ã€‚
    """
    try:
        additional_context = [{'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']} for msg in additional_context]
        combined_context = "\n\n".join([doc["content"] for doc in documents_cn + documents_en])
        prompt = f"""ç›¸é—œæ–‡ç« ï¼š
{combined_context}

ç”¨æˆ¶å•é¡Œï¼š
{user_query}

å›ç­”ï¼š"""
        # ä½¿ç”¨ Anthropic API é€²è¡Œå›ç­”
        message = claude_client.messages.create(
            model=model,
            max_tokens=8192,
            temperature=0.6,  # èª¿æ•´æ­¤è™•çš„ temperature
            system="""ä½ æ˜¯ä¸€å€‹é£½å­¸æ€§çŸ¥è­˜çš„å°ˆå®¶ï¼Œè² è²¬èˆ‡ç”¨æˆ¶çœŸèª çš„èŠå¤©!
            éç¨‹è¦ä¿æŒç†±æƒ…ã€å‹å–„ä¸”å…·æœ‰åŒç†å¿ƒğŸ’›""",
            messages= additional_context + [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        return message.content[0].text

    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return ""

def is_sex_related(query: str, claude_client, model) -> bool:
    """
    åˆ¤æ–·å•é¡Œæ˜¯å¦èˆ‡æ€§ç›¸é—œã€‚
    
    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        claude_client: Anthropic çš„ API å®¢æˆ¶ç«¯ã€‚
        model (str): Anthropic æ¨¡å‹åç¨±ã€‚

    Returns:
        bool: å¦‚æœå•é¡Œèˆ‡æ€§ç›¸é—œï¼Œè¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
    """
    prompt = f"å•é¡Œï¼š{query}\nå›è¦†ï¼š"
    try:
        response = claude_client.messages.create(
            model=model,
            max_tokens=3,
            temperature=0.4,  # èª¿æ•´æ­¤è™•çš„ temperature
            system="è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ˜¯å¦èˆ‡æ€§æˆ–æ€§çŸ¥è­˜æˆ–èº«é«”æœ‰ä»»ä½•é—œè¯ã€‚è«‹åƒ…å›è¦†ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        answer = str(response.content[0].text).strip()
        return answer == "æ˜¯"
    except Exception as e:
        logger.error(f"åˆ¤æ–·æ€§ç›¸é—œæ€§æ™‚å‡ºéŒ¯ï¼š{e}")
        # é è¨­å‡è¨­ç‚ºéæ€§ç›¸é—œå•é¡Œ
        return False 

def generate_direct_response(user_query: str, additional_context: list, claude_client, model: str) -> str:
    """
    ç”Ÿæˆç›´æ¥å›ç­”ï¼Œç„¡éœ€æª¢ç´¢ç›¸é—œæ–‡æª”ã€‚
    
    Args:
        user_query (str): ä½¿ç”¨è€…çš„æŸ¥è©¢ã€‚
        additional_context (list): é¡å¤–ä¸Šä¸‹æ–‡ã€‚
        claude_client: Anthropic çš„ API å®¢æˆ¶ç«¯ã€‚
        model (str): Anthropic æ¨¡å‹åç¨±ã€‚
    
    Returns:
        str: ç”Ÿæˆçš„å›ç­”ã€‚
    """
    additional_context = [{'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']} for msg in additional_context]
    prompt = (
        f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}\n"
        "è«‹æä¾›ç›¸æ‡‰çš„å›ç­”ã€‚"
    )
    try:
        response = claude_client.messages.create(
            model=model,
            max_tokens=8192,
            temperature=0.6,  # èª¿æ•´æ­¤è™•çš„ temperature
            system="ä½ æ˜¯ä¸€å€‹é£½å­¸æ€§çŸ¥è­˜çš„å°ˆå®¶ï¼Œè² è²¬èˆ‡ç”¨æˆ¶çœŸèª çš„èŠå¤©! éç¨‹è¦ä¿æŒç†±æƒ…ã€å‹å–„ä¸”å…·æœ‰åŒç†å¿ƒğŸ’›",
            messages= additional_context + [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        answer = response.content[0].text
        return answer
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯ï¼š{e}")
        return "" 