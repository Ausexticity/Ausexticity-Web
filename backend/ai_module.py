import os
import json
from dotenv import load_dotenv
from google.cloud import bigquery
from vertexai.language_models import TextEmbeddingModel
import vertexai
from google.cloud import translate_v2 as translate
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
from openai import OpenAI  # æ–°ç”¨æ³•

# åˆå§‹åŒ– Vertex AI
vertexai.init(project="eros-ai-446307", location="us-central1")

# åˆå§‹åŒ–ç¿»è­¯å®¢æˆ¶ç«¯
translate_client = translate.Client()

# åˆå§‹åŒ– ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=10)  # æ ¹æ“šéœ€æ±‚èª¿æ•´ max_workers

# åˆå§‹åŒ– Logger
logger = logging.getLogger('uvicorn.error')

# å»ºç«‹ OpenAI å®¢æˆ¶ç«¯ï¼Œä½¿ç”¨ OpenRouter API
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=os.getenv("OPENROUTER_API_KEY"),
)

# å®šç¾©é¡å¤–è«‹æ±‚æ¨™é ­ï¼ˆå¯ä¾éœ€æ±‚è¨­å®šï¼‰
EXTRA_HEADERS = {
    "HTTP-Referer": os.getenv("SITE_URL", "https://ausexticity.com"),  # é¸å¡«ï¼šæ‚¨çš„ç¶²ç«™ URL
    "X-Title": os.getenv("SITE_NAME", "Ausexticity")  # é¸å¡«ï¼šæ‚¨çš„ç¶²ç«™åç¨±
}

def load_query_embedding_sync(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    model = TextEmbeddingModel.from_pretrained(model_name)
    embeddings = model.get_embeddings([query])
    return embeddings[0].values

async def load_query_embedding_async(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆéåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        load_query_embedding_sync,
        query,
        model_name
    )

def retrieve_similar_documents_sync(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    åŒæ­¥ä½¿ç”¨ BigQuery æŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    client_bq = bigquery.Client(project=project_id)
    embedding_str = ", ".join(map(str, query_embedding))
    query = f"""
    WITH query AS (
      SELECT
        [{embedding_str}] AS embedding
    )
    SELECT
      t.id,
      t.title,
      t.url,
      t.content,
      t.embedding,
      (
        (SELECT SUM(q * d) 
         FROM UNNEST(qry.embedding) AS q WITH OFFSET i 
         JOIN UNNEST(t.embedding) AS d WITH OFFSET j 
         ON i = j)
        /
        (SQRT((SELECT SUM(POWER(q, 2)) FROM UNNEST(qry.embedding) AS q)) * 
         SQRT((SELECT SUM(POWER(d, 2)) FROM UNNEST(t.embedding) AS d)))
      ) AS cosine_similarity
    FROM
      `{project_id}.{dataset_id}.{table_id}` AS t,
      query AS qry
    ORDER BY
      cosine_similarity DESC
    LIMIT {top_n}
    """
    query_job = client_bq.query(query)
    results = query_job.result()
    documents = []
    for row in results:
        documents.append({
            "id": row.id,
            "title": row.title,
            "url": row.url,
            "content": row.content,
            "cosine_similarity": row.cosine_similarity
        })
    client_bq.close()
    return documents

async def retrieve_similar_documents_async(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ BigQuery æŸ¥è©¢ï¼ŒæŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        retrieve_similar_documents_sync,
        query_embedding,
        dataset_id,
        table_id,
        project_id,
        top_n
    )

def translate_text_sync(text):
    """
    åŒæ­¥ä½¿ç”¨ Google Translate API å°‡æ–‡æœ¬ç¿»è­¯ç‚ºè‹±æ–‡ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    try:
        result = translate_client.translate(text, target_language='en')
        return result['translatedText']
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—ï¼š{str(e)}")
        return ""

async def translate_text_async(text):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ç¿»è­¯å‡½å¼ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        translate_text_sync,
        text
    )

def generate_response(documents_cn, documents_en, user_query, additional_context, model, web_search: bool = False):
    """
    ä½¿ç”¨æª¢ç´¢åˆ°çš„ä¸­è‹±æ–‡æ–‡æª”èˆ‡ç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆå›ç­”ï¼Œæ¡ç”¨ä¸²æµå¼æ–¹å¼æ¥æ”¶å…§å®¹ã€‚

    Args:
        documents_cn (list): ä¸­æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        documents_en (list): è‹±æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        user_query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        additional_context (list): é¡å¤–çš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚
        web_search (bool): æ˜¯å¦å•Ÿç”¨ç¶²è·¯æœå°‹åŠŸèƒ½ï¼Œé è¨­ç‚º Falseã€‚

    Returns:
        str: ç”Ÿæˆçš„å›ç­”ï¼ˆå®Œæ•´çš„æ–‡å­—å…§å®¹ï¼‰ã€‚
    """
    try:
        additional_context = [
            {'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']}
            for msg in additional_context
        ]
        combined_context = "\n\n".join([doc["content"] for doc in documents_cn + documents_en])
        prompt = f"""ç›¸é—œæ–‡ç« ï¼š
{combined_context}

ä½¿ç”¨ç”Ÿå‹•æœ‰è¶£çš„æ–¹å¼ä»‹ç´¹æ€§çŸ¥è­˜ï¼Œä¸¦ä¸”é©æ™‚çš„ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢æ·»è¶£å‘³æ€§ğŸ‘ğŸ’¦ å› èŠå¤©ä»‹é¢å·²åŒ…å«é‡è¦æé†’: 1.éœ€è¦æ³¨æ„è¡›ç”Ÿå’Œå®‰å…¨ 2.æ‡‰è©²å°Šé‡é›™æ–¹æ„é¡˜ 3.ä¿æŒé–‹æ”¾æºé€šå¾ˆé‡è¦ï¼Œä¸éœ€è¦åœ¨è¨Šæ¯ä¸­å†æ¬¡æé†’

ç”¨æˆ¶å•é¡Œï¼š
{user_query}
"""
        params = {
            "extra_headers": EXTRA_HEADERS,
            "model": model,
            "max_tokens": 8192,
            "temperature": 0.8,
            "messages": additional_context + [
                {"role": "user", "content": prompt}
            ],
            "stream": True  # å•Ÿç”¨ä¸²æµæ¨¡å¼
        }
        if web_search:
            params["plugins"] = [{"id": "web"}]
        response = client.chat.completions.create(**params)
        full_response = ""
        for chunk in response:
            delta = chunk.choices[0].delta
            full_response += delta.get("content", "")
        return full_response
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return ""

def is_sex_related(query: str, model) -> bool:
    """
    åˆ¤æ–·å•é¡Œæ˜¯å¦èˆ‡æ€§ç›¸é—œï¼Œä½¿ç”¨ä¸²æµæ¨¡å¼ä»¥å–å¾—å›æ‡‰å¾Œåˆ¤æ–·ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚

    Returns:
        bool: å¦‚æœå•é¡Œèˆ‡æ€§ç›¸é—œï¼Œè¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
    """
    prompt = f"å•é¡Œï¼š{query}\nå›è¦†ï¼š"
    try:
        response = client.chat.completions.create(
            extra_headers=EXTRA_HEADERS,
            model=model,
            max_tokens=3,
            temperature=0.4,
            messages=[
                {"role": "system", "content": "è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ˜¯å¦èˆ‡æ€§æˆ–æ€§çŸ¥è­˜æˆ–èº«é«”æœ‰ä»»ä½•é—œè¯ã€‚è«‹åƒ…å›è¦†ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=True  # ä¸²æµæ¨¡å¼
        )
        answer = ""
        for chunk in response:
            delta = chunk.choices[0].delta
            answer += delta.get("content", "")
        return answer.strip() == "æ˜¯"
    except Exception as e:
        logger.error(f"åˆ¤æ–·æ€§ç›¸é—œæ€§æ™‚å‡ºéŒ¯ï¼š{e}")
        return False 

def generate_direct_response(user_query: str, additional_context: list, model: str, web_search: bool = False) -> str:
    """
    ç”Ÿæˆç›´æ¥å›ç­”ï¼Œç„¡éœ€æª¢ç´¢ç›¸é—œæ–‡æª”ï¼Œæ¡ç”¨ä¸²æµæ¨¡å¼æ¥æ”¶å›ç­”ã€‚

    Args:
        user_query (str): ä½¿ç”¨è€…çš„æŸ¥è©¢ã€‚
        additional_context (list): é¡å¤–ä¸Šä¸‹æ–‡ã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚
        web_search (bool): æ˜¯å¦å•Ÿç”¨ç¶²è·¯æœå°‹åŠŸèƒ½ï¼Œé è¨­ç‚º Falseã€‚

    Returns:
        str: ç”Ÿæˆçš„å›ç­”ã€‚
    """
    additional_context = [
        {'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']}
        for msg in additional_context
    ]
    prompt = f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}\nè«‹æä¾›ç›¸æ‡‰çš„å›ç­”ã€‚"
    try:
        params = {
            "extra_headers": EXTRA_HEADERS,
            "model": model,
            "max_tokens": 8192,
            "temperature": 0.6,
            "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€å€‹é£½å­¸æ€§çŸ¥è­˜çš„å°ˆå®¶ï¼Œè² è²¬èˆ‡ç”¨æˆ¶çœŸèª çš„èŠå¤©! éç¨‹è¦ä¿æŒç†±æƒ…ã€å‹å–„ä¸”å…·æœ‰åŒç†å¿ƒğŸ’›"}] + additional_context + [
                {"role": "user", "content": prompt}
            ],
            "stream": True  # å•Ÿç”¨ä¸²æµæ¨¡å¼
        }
        if web_search:
            params["plugins"] = [{"id": "web"}]
        response = client.chat.completions.create(**params)
        full_response = ""
        for chunk in response:
            delta = chunk.choices[0].delta
            full_response += delta.get("content", "")
        return full_response
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯ï¼š{e}")
        return "" 