import os
import json
from dotenv import load_dotenv
from google.cloud import bigquery
from vertexai.language_models import TextEmbeddingModel
import vertexai
from google.cloud import translate_v2 as translate
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging
from openai import OpenAI  # æ–°ç”¨æ³•

# å¾ç’°å¢ƒè®Šæ•¸ç²å–ä¸¦è§£æ Google æ†‘è­‰
credentials_dict = json.loads(os.getenv('GOOGLE_CREDENTIALS2'))

# åˆå§‹åŒ– Vertex AI
vertexai.init(project="eros-ai-446307", location="us-central1", credentials=credentials_dict)

# åˆå§‹åŒ–ç¿»è­¯å®¢æˆ¶ç«¯
translate_client = translate.Client(credentials=credentials_dict)

# åˆå§‹åŒ– ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=4)  # å¾ 10 é™åˆ° 4

# åˆå§‹åŒ– Logger
logger = logging.getLogger('uvicorn.error')

# å»ºç«‹ OpenAI å®¢æˆ¶ç«¯ï¼Œä½¿ç”¨ OpenRouter API
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=os.getenv("OPENROUTER_API_KEY"),
)

# å®šç¾©é¡å¤–è«‹æ±‚æ¨™é ­ï¼ˆå¯ä¾éœ€æ±‚è¨­å®šï¼‰
EXTRA_HEADERS = {
    "HTTP-Referer": os.getenv("SITE_URL", "https://ausexticity.com"),  # é¸å¡«ï¼šæ‚¨çš„ç¶²ç«™ URL
    "X-Title": os.getenv("SITE_NAME", "Ausexticity")  # é¸å¡«ï¼šæ‚¨çš„ç¶²ç«™åç¨±
}

def load_query_embedding_sync(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    model = TextEmbeddingModel.from_pretrained(model_name)
    embeddings = model.get_embeddings([query])
    return embeddings[0].values

async def load_query_embedding_async(query, model_name="text-multilingual-embedding-002"):
    """
    ä½¿ç”¨ Vertex AI åµŒå…¥æ¨¡å‹å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ï¼ˆéåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model_name (str): åµŒå…¥æ¨¡å‹åç¨±ã€‚

    Returns:
        list: æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        load_query_embedding_sync,
        query,
        model_name
    )

def retrieve_similar_documents_sync(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    åŒæ­¥ä½¿ç”¨ BigQuery æŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    client_bq = bigquery.Client(project=project_id)
    embedding_str = ", ".join(map(str, query_embedding))
    query = f"""
    WITH query AS (
      SELECT
        [{embedding_str}] AS embedding
    )
    SELECT
      t.id,
      t.title,
      t.url,
      t.content,
      t.embedding,
      (
        (SELECT SUM(q * d) 
         FROM UNNEST(qry.embedding) AS q WITH OFFSET i 
         JOIN UNNEST(t.embedding) AS d WITH OFFSET j 
         ON i = j)
        /
        (SQRT((SELECT SUM(POWER(q, 2)) FROM UNNEST(qry.embedding) AS q)) * 
         SQRT((SELECT SUM(POWER(d, 2)) FROM UNNEST(t.embedding) AS d)))
      ) AS cosine_similarity
    FROM
      `{project_id}.{dataset_id}.{table_id}` AS t,
      query AS qry
    ORDER BY
      cosine_similarity DESC
    LIMIT {top_n}
    """
    query_job = client_bq.query(query)
    results = query_job.result()
    documents = []
    for row in results:
        documents.append({
            "id": row.id,
            "title": row.title,
            "url": row.url,
            "content": row.content,
            "cosine_similarity": row.cosine_similarity
        })
    client_bq.close()
    return documents

async def retrieve_similar_documents_async(query_embedding, dataset_id, table_id, project_id, top_n=10):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ BigQuery æŸ¥è©¢ï¼ŒæŸ¥æ‰¾èˆ‡æŸ¥è©¢å‘é‡ç›¸ä¼¼çš„æ–‡æª”ã€‚

    Args:
        query_embedding (list): æŸ¥è©¢çš„åµŒå…¥å‘é‡ã€‚
        dataset_id (str): BigQuery è³‡æ–™é›† IDã€‚
        table_id (str): BigQuery è³‡æ–™è¡¨ IDã€‚
        project_id (str): GCP å°ˆæ¡ˆ IDã€‚
        top_n (int): è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ã€‚

    Returns:
        list: ç›¸ä¼¼æ–‡æª”çš„åˆ—è¡¨ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        retrieve_similar_documents_sync,
        query_embedding,
        dataset_id,
        table_id,
        project_id,
        top_n
    )

def translate_text_sync(text):
    """
    åŒæ­¥ä½¿ç”¨ Google Translate API å°‡æ–‡æœ¬ç¿»è­¯ç‚ºè‹±æ–‡ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    try:
        result = translate_client.translate(text, target_language='en')
        return result['translatedText']
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—ï¼š{str(e)}")
        return ""

async def translate_text_async(text):
    """
    éåŒæ­¥ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡ŒåŒæ­¥ç¿»è­¯å‡½å¼ã€‚

    Args:
        text (str): åŸå§‹æ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰ã€‚

    Returns:
        str: ç¿»è­¯å¾Œçš„è‹±æ–‡æ–‡æœ¬ã€‚
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        translate_text_sync,
        text
    )



# åœ¨æª”æ¡ˆæœ«å°¾æ–°å¢ä»¥ä¸‹ SSE ç‰ˆæœ¬çš„æµå¼å›æ‡‰ç›¸é—œå‡½å¼

async def stream_response_async(params):
    """
    éåŒæ­¥åœ°å¾ OpenAI API å–å¾—ä¸²æµå›æ‡‰ï¼Œä¸¦åˆ©ç”¨ asyncio.Queue å°‡åŒæ­¥è¿­ä»£åŒ…è£æˆéåŒæ­¥ç”¢ç”Ÿå™¨ã€‚
    """
    loop = asyncio.get_event_loop()
    q = asyncio.Queue()

    def run():
        try:
            # ä½¿ç”¨ OpenAI API ç”¢ç”Ÿå™¨é€å¡Šè®€å–å›æ‡‰
            for chunk in client.chat.completions.create(**params):
                # ç”±æ–¼ delta ç¾åœ¨æ˜¯ä¸€å€‹ç‰©ä»¶ï¼Œå› æ­¤ä½¿ç”¨å±¬æ€§å­˜å–
                delta = chunk.choices[0].delta
                content = delta.content if hasattr(delta, "content") else ""
                asyncio.run_coroutine_threadsafe(q.put(content), loop)
        except Exception as e:
            logger.error(f"æµå¼å›æ‡‰éŒ¯èª¤: {e}")
        finally:
            asyncio.run_coroutine_threadsafe(q.put(None), loop)

    executor.submit(run)

    while True:
        content = await q.get()
        if content is None:
            break
        yield content
        
def is_sex_related(query: str, model) -> bool:
    """
    åˆ¤æ–·å•é¡Œæ˜¯å¦èˆ‡æ€§ç›¸é—œï¼Œä½¿ç”¨ä¸²æµæ¨¡å¼ä»¥å–å¾—å›æ‡‰å¾Œåˆ¤æ–·ã€‚

    Args:
        query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚

    Returns:
        bool: å¦‚æœå•é¡Œèˆ‡æ€§ç›¸é—œï¼Œè¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
    """
    prompt = f"å•é¡Œï¼š{query}\nå›è¦†ï¼š"
    try:
        response = client.chat.completions.create(
            extra_headers=EXTRA_HEADERS,
            model=model,
            max_tokens=3,
            temperature=0.4,
            messages=[
                {"role": "system", "content": "è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ˜¯å¦èˆ‡æ€§æˆ–æ€§çŸ¥è­˜æˆ–èº«é«”æœ‰ä»»ä½•é—œè¯ã€‚è«‹åƒ…å›è¦†ã€Œæ˜¯ã€æˆ–ã€Œå¦ã€ã€‚"},
                {"role": "user", "content": prompt}
            ],
        )
        answer = response.choices[0].message.content
        return answer.strip() == "æ˜¯"
    except Exception as e:
        logger.error(f"åˆ¤æ–·æ€§ç›¸é—œæ€§æ™‚å‡ºéŒ¯ï¼š{e}")
        return False 


async def generate_response_stream(documents_cn, documents_en, user_query, additional_context, model, web_search: bool = False):
    """
    ä½¿ç”¨æª¢ç´¢åˆ°çš„ä¸­è‹±æ–‡æ–‡æª”èˆ‡ç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆå›ç­”ï¼Œä¸¦ä»¥ SSE æµå¼å›å‚³çµæœã€‚
    
    Args:
        documents_cn (list): ä¸­æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        documents_en (list): è‹±æ–‡æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨ã€‚
        user_query (str): ç”¨æˆ¶çš„æŸ¥è©¢ã€‚
        additional_context (list): é¡å¤–çš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚
        web_search (bool): æ˜¯å¦å•Ÿç”¨ç¶²è·¯æœå°‹åŠŸèƒ½ï¼Œé è¨­ç‚º Falseã€‚
    
    Yields:
        dict: åŒ…å« SSE äº‹ä»¶çš„è¨Šæ¯ã€‚
    """
    additional_context = [
        {'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']}
        for msg in additional_context
    ]
    combined_context = "\n\n".join([doc["content"] for doc in documents_cn + documents_en])
    prompt = f"""ç›¸é—œæ–‡ç« ï¼š
{combined_context}

ä½¿ç”¨ç”Ÿå‹•æœ‰è¶£çš„æ–¹å¼ä»‹ç´¹æ€§çŸ¥è­˜ï¼Œä¸¦ä¸”é©æ™‚çš„ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢æ·»è¶£å‘³æ€§ğŸ‘ğŸ’¦ å› èŠå¤©ä»‹é¢å·²åŒ…å«é‡è¦æé†’: 1.éœ€è¦æ³¨æ„è¡›ç”Ÿå’Œå®‰å…¨ 2.æ‡‰è©²å°Šé‡é›™æ–¹æ„é¡˜ 3.ä¿æŒé–‹æ”¾æºé€šå¾ˆé‡è¦ï¼Œä¸éœ€è¦åœ¨è¨Šæ¯ä¸­å†æ¬¡æé†’

ç”¨æˆ¶å•é¡Œï¼š
{user_query}
"""
    # è‹¥å•Ÿç”¨ç¶²è·¯æœå°‹åŠŸèƒ½ï¼Œå‰‡æ–¼æ¨¡å‹åç¨±å¾Œé™„ä¸Š ":online"
    if web_search:
        if not model.endswith(":online"):
            model = f"{model}:online"
    params = {
        "extra_headers": EXTRA_HEADERS,
        "model": model,
        "max_tokens": 8192,
        "temperature": 0.8,
        "messages": additional_context + [
            {"role": "user", "content": prompt}
        ],
        "stream": True
    }
    
    async for chunk in stream_response_async(params):
        yield {"event": "message", "data": chunk}
    yield {"event": "end", "data": ""}

async def generate_direct_response_stream(user_query: str, additional_context: list, model: str, web_search: bool = False):
    """
    ç”Ÿæˆç›´æ¥å›ç­”ï¼Œç„¡éœ€æª¢ç´¢ç›¸é—œæ–‡æª”ï¼Œä¸¦ä»¥ SSE æµå¼å›å‚³çµæœã€‚
    
    Args:
        user_query (str): ä½¿ç”¨è€…çš„æŸ¥è©¢ã€‚
        additional_context (list): é¡å¤–ä¸Šä¸‹æ–‡è³‡è¨Šã€‚
        model (str): ä½¿ç”¨çš„æ¨¡å‹ã€‚
        web_search (bool): æ˜¯å¦å•Ÿç”¨ç¶²è·¯æœå°‹åŠŸèƒ½ï¼Œé è¨­ç‚º Falseã€‚
    
    Yields:
        dict: åŒ…å« SSE äº‹ä»¶çš„è¨Šæ¯ã€‚
    """
    additional_context = [
        {'role': 'assistant' if msg['is_bot'] else 'user', 'content': msg['message']}
        for msg in additional_context
    ]
    prompt = f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}\nè«‹æä¾›ç›¸æ‡‰çš„å›ç­”ã€‚"
    if web_search:
        if not model.endswith(":online"):
            model = f"{model}:online"
    params = {
        "extra_headers": EXTRA_HEADERS,
        "model": model,
        "max_tokens": 8192,
        "temperature": 0.6,
        "messages": [{"role": "system", "content": "ä½¿ç”¨ç”Ÿå‹•æœ‰è¶£çš„æ–¹å¼ä»‹ç´¹æ€§çŸ¥è­˜ï¼Œä¸¦ä¸”é©æ™‚çš„ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢æ·»è¶£å‘³æ€§ğŸ‘ğŸ’¦ å› èŠå¤©ä»‹é¢å·²åŒ…å«é‡è¦æé†’: 1.éœ€è¦æ³¨æ„è¡›ç”Ÿå’Œå®‰å…¨ 2.æ‡‰è©²å°Šé‡é›™æ–¹æ„é¡˜ 3.ä¿æŒé–‹æ”¾æºé€šå¾ˆé‡è¦ï¼Œä¸éœ€è¦åœ¨è¨Šæ¯ä¸­å†æ¬¡æé†’"}] + additional_context + [
            {"role": "user", "content": prompt}
        ],
        "stream": True
    }
    
    async for chunk in stream_response_async(params):
        yield {"event": "message", "data": chunk}
    yield {"event": "end", "data": ""} 